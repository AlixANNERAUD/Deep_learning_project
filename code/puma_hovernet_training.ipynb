{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b91849",
   "metadata": {},
   "source": [
    "# PUMA Challenge - Training HoVer-Net\n",
    "\n",
    "This notebook demonstrates how to train the HoVer-Net model for the Panoptic Segmentation of nUclei and tissue in advanced MelanomA (PUMA) challenge. \n",
    "\n",
    "## PUMA Challenge Overview\n",
    "\n",
    "The PUMA challenge consists of two tracks, each with two tasks:\n",
    "\n",
    "### Track 1 – Panoptic segmentation with three instance classes:\n",
    "- **Task 1**: Semantic tissue segmentation of tumor, stroma, epithelium, blood vessel, and necrotic regions.\n",
    "- **Task 2**: Nuclei detection for three classes; tumor, TILs (lymphocytes and plasma cells), and other cells.\n",
    "\n",
    "### Track 2 – Panoptic segmentation with ten instance classes:\n",
    "- **Task 1**: Semantic tissue segmentation (same as Track 1).\n",
    "- **Task 2**: Nuclei detection for all ten classes: tumor, lymphocytes, plasma cells, histiocytes, melanophages, neutrophils, stromal cells, epithelium, endothelium, and apoptotic cells.\n",
    "\n",
    "In this notebook, we'll focus on training HoVer-Net for both tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01efdc",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's set up our environment and import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "769f4e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add HoVer-Net directory to path\n",
    "sys.path.append('./hover_net/')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8b8d6",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Understanding\n",
    "\n",
    "The PUMA dataset consists of:\n",
    "- 155 primary and 155 metastatic melanoma regions of interest (ROI), scanned at 40x magnification (1024 x 1024 pixels)\n",
    "- Context ROI of 5120 x 5120 pixels, centered around each ROI\n",
    "- Annotations for tissue and nuclei\n",
    "\n",
    "Let's explore the dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "912bd4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nuclei annotation files: 206\n",
      "Number of tissue annotation files: 206\n",
      "Number of ROI image files: 206\n",
      "Number of context ROI image files: 205\n",
      "\n",
      "Sample nuclei annotation files:\n",
      "  - training_set_metastatic_roi_028_nuclei.geojson\n",
      "  - training_set_primary_roi_086_nuclei.geojson\n",
      "  - training_set_primary_roi_093_nuclei.geojson\n",
      "  - training_set_primary_roi_008_nuclei.geojson\n",
      "  - training_set_metastatic_roi_059_nuclei.geojson\n"
     ]
    }
   ],
   "source": [
    "# Define dataset paths\n",
    "dataset_path = \"../dataset\"\n",
    "\n",
    "# List available ROIs\n",
    "nuclei_annotations_path = os.path.join(dataset_path, \"01_training_dataset_geojson_nuclei\")\n",
    "tissue_annotations_path = os.path.join(dataset_path, \"01_training_dataset_geojson_tissue\")\n",
    "roi_images_path = os.path.join(dataset_path, \"01_training_dataset_tif_ROIs\")\n",
    "context_roi_path = os.path.join(dataset_path, \"01_training_dataset_tif_context_ROIs\")\n",
    "\n",
    "# Count the number of files in each directory\n",
    "print(f\"Number of nuclei annotation files: {len(os.listdir(nuclei_annotations_path))}\")\n",
    "print(f\"Number of tissue annotation files: {len(os.listdir(tissue_annotations_path))}\")\n",
    "print(f\"Number of ROI image files: {len(os.listdir(roi_images_path))}\")\n",
    "print(f\"Number of context ROI image files: {len(os.listdir(context_roi_path))}\")\n",
    "\n",
    "# Look at sample filenames\n",
    "print(\"\\nSample nuclei annotation files:\")\n",
    "for file in os.listdir(nuclei_annotations_path)[:5]:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2195285",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "HoVer-Net requires the data in a specific format. For training, we need to:\n",
    "1. Convert GeoJSON annotations to instance masks\n",
    "2. Extract patches from original images\n",
    "3. Format data as required by HoVer-Net: [RGB, inst, type] channels\n",
    "\n",
    "Let's create functions to handle this conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7bd03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point\n",
    "import tifffile\n",
    "\n",
    "def read_geojson(geojson_path):\n",
    "    \"\"\"Read a GeoJSON file containing annotations.\"\"\"\n",
    "    with open(geojson_path) as f:\n",
    "        geojson = json.load(f)\n",
    "    return geojson\n",
    "\n",
    "def geojson_to_mask(geojson, img_shape, class_mapping=None):\n",
    "    \"\"\"Convert GeoJSON annotations to instance and type masks.\"\"\"\n",
    "    instance_mask = np.zeros(img_shape[:2], dtype=np.int32)\n",
    "    type_mask = np.zeros(img_shape[:2], dtype=np.int32)\n",
    "    \n",
    "    instance_id = 1  # Start with ID 1 (0 is background)\n",
    "    \n",
    "    for feature in geojson['features']:\n",
    "        cell_type = feature['properties'].get('classification', {}).get('name', 'unknown')\n",
    "        type_id = class_mapping.get(cell_type, 0) if class_mapping else 1\n",
    "        \n",
    "        # Get polygon coordinates\n",
    "        if feature['geometry']['type'] == 'Polygon':\n",
    "            coords = feature['geometry']['coordinates'][0]\n",
    "            # Convert to integer coordinates for cv2\n",
    "            coords = np.array(coords, dtype=np.int32)\n",
    "            \n",
    "            # Create instance mask\n",
    "            cv2.fillPoly(instance_mask, [coords], instance_id)\n",
    "            \n",
    "            # Create type mask for the same region\n",
    "            cv2.fillPoly(type_mask, [coords], type_id)\n",
    "            \n",
    "            instance_id += 1\n",
    "    \n",
    "    return instance_mask, type_mask\n",
    "\n",
    "def extract_patches(image, instance_mask, type_mask, patch_size=270, stride=80):\n",
    "    \"\"\"Extract patches from image and corresponding masks.\"\"\"\n",
    "    patches = []\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            img_patch = image[y:y+patch_size, x:x+patch_size].copy()\n",
    "            inst_patch = instance_mask[y:y+patch_size, x:x+patch_size].copy()\n",
    "            type_patch = type_mask[y:y+patch_size, x:x+patch_size].copy()\n",
    "            \n",
    "            # Only keep patches with some nuclei\n",
    "            if np.max(inst_patch) > 0:\n",
    "                # Relabel instance IDs to be consecutive starting from 1\n",
    "                unique_ids = np.unique(inst_patch)\n",
    "                unique_ids = unique_ids[unique_ids > 0]  # Skip background\n",
    "                mapping = {old_id: new_id for new_id, old_id in enumerate(unique_ids, 1)}\n",
    "                mapping[0] = 0  # Keep background as 0\n",
    "                \n",
    "                for old_id, new_id in mapping.items():\n",
    "                    inst_patch[inst_patch == old_id] = new_id\n",
    "                \n",
    "                # Store patch with format [RGB, inst, type]\n",
    "                patch_data = np.dstack([img_patch, inst_patch[..., None], type_patch[..., None]])\n",
    "                patches.append(patch_data)\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33757819",
   "metadata": {},
   "source": [
    "### Define Nuclear Type Mapping\n",
    "\n",
    "We need to define how cell types in the PUMA dataset map to type IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1480c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track 1 (3 classes) mapping\n",
    "track1_mapping = {\n",
    "    'tumor': 1,\n",
    "    'lymphocyte': 2,  # TIL\n",
    "    'plasma': 2,      # TIL\n",
    "    'histiocyte': 3,  # Other\n",
    "    'melanophage': 3, # Other\n",
    "    'neutrophil': 3,  # Other\n",
    "    'stromal': 3,     # Other\n",
    "    'epithelium': 3,  # Other\n",
    "    'endothelium': 3, # Other\n",
    "    'apoptotic': 3,   # Other\n",
    "    'unknown': 0      # Background\n",
    "}\n",
    "\n",
    "# Track 2 (10 classes) mapping\n",
    "track2_mapping = {\n",
    "    'tumor': 1,\n",
    "    'lymphocyte': 2,\n",
    "    'plasma': 3,\n",
    "    'histiocyte': 4,\n",
    "    'melanophage': 5,\n",
    "    'neutrophil': 6,\n",
    "    'stromal': 7,\n",
    "    'epithelium': 8,\n",
    "    'endothelium': 9,\n",
    "    'apoptotic': 10,\n",
    "    'unknown': 0     # Background\n",
    "}\n",
    "\n",
    "# For tissue segmentation\n",
    "tissue_mapping = {\n",
    "    'tumor': 1,\n",
    "    'stroma': 2,\n",
    "    'epithelium': 3,\n",
    "    'blood_vessel': 4,\n",
    "    'necrosis': 5,\n",
    "    'unknown': 0     # Background\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be6096",
   "metadata": {},
   "source": [
    "### Process Data to HoVer-Net Format\n",
    "\n",
    "Let's create a function to process the dataset and save the patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "670518e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(output_dir, class_mapping, track_name=\"track1\"):\n",
    "    \"\"\"Process the PUMA dataset and create patches for HoVer-Net training.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get list of ROI images\n",
    "    roi_files = sorted(glob.glob(os.path.join(roi_images_path, \"*.tif\")))\n",
    "    \n",
    "    for i, roi_file in enumerate(roi_files):\n",
    "        roi_filename = os.path.basename(roi_file)\n",
    "        roi_id = roi_filename.split('.')[0]  # Extract ID without extension\n",
    "        \n",
    "        print(f\"Processing {i+1}/{len(roi_files)}: {roi_id}\")\n",
    "        \n",
    "        # Find corresponding annotation file\n",
    "        nuclei_geojson_file = os.path.join(nuclei_annotations_path, f\"{roi_id}_nuclei.geojson\")\n",
    "        \n",
    "        if not os.path.exists(nuclei_geojson_file):\n",
    "            print(f\"WARNING: No annotation found for {roi_id}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Load image and annotations\n",
    "        image = tifffile.imread(roi_file)\n",
    "        geojson = read_geojson(nuclei_geojson_file)\n",
    "        \n",
    "        # Convert annotations to masks\n",
    "        instance_mask, type_mask = geojson_to_mask(geojson, image.shape, class_mapping)\n",
    "        \n",
    "        # Extract patches\n",
    "        patches = extract_patches(image, instance_mask, type_mask)\n",
    "        \n",
    "        # Save patches\n",
    "        for j, patch in enumerate(patches):\n",
    "            patch_filename = f\"{roi_id}_patch_{j}.npy\"\n",
    "            np.save(os.path.join(output_dir, patch_filename), patch)\n",
    "            \n",
    "        # If processing too many ROIs at once, consider adding a limit\n",
    "        # if i >= 10:\n",
    "        #     break\n",
    "    \n",
    "    print(f\"Finished processing {track_name}. Total patches saved: {len(glob.glob(os.path.join(output_dir, '*.npy')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c194fef",
   "metadata": {},
   "source": [
    "### Run Data Processing\n",
    "\n",
    "Let's create patches for both Track 1 and Track 2. Note that this may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f409a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/205: training_set_metastatic_roi_001\n",
      "Processing 2/205: training_set_metastatic_roi_002\n",
      "Processing 3/205: training_set_metastatic_roi_003\n",
      "Processing 4/205: training_set_metastatic_roi_004\n",
      "Processing 5/205: training_set_metastatic_roi_005\n",
      "Processing 6/205: training_set_metastatic_roi_006\n",
      "Processing 7/205: training_set_metastatic_roi_007\n",
      "Processing 8/205: training_set_metastatic_roi_008\n",
      "Processing 9/205: training_set_metastatic_roi_009\n",
      "Processing 10/205: training_set_metastatic_roi_010\n",
      "Processing 11/205: training_set_metastatic_roi_011\n",
      "Processing 12/205: training_set_metastatic_roi_012\n",
      "Processing 13/205: training_set_metastatic_roi_013\n",
      "Processing 14/205: training_set_metastatic_roi_014\n",
      "Processing 15/205: training_set_metastatic_roi_015\n",
      "Processing 16/205: training_set_metastatic_roi_016\n",
      "Processing 17/205: training_set_metastatic_roi_017\n",
      "Processing 18/205: training_set_metastatic_roi_018\n",
      "Processing 19/205: training_set_metastatic_roi_019\n",
      "Processing 20/205: training_set_metastatic_roi_020\n",
      "Processing 21/205: training_set_metastatic_roi_021\n",
      "Processing 22/205: training_set_metastatic_roi_022\n",
      "Processing 23/205: training_set_metastatic_roi_023\n",
      "Processing 24/205: training_set_metastatic_roi_024\n",
      "Processing 25/205: training_set_metastatic_roi_025\n",
      "Processing 26/205: training_set_metastatic_roi_026\n",
      "Processing 27/205: training_set_metastatic_roi_027\n",
      "Processing 28/205: training_set_metastatic_roi_028\n",
      "Processing 29/205: training_set_metastatic_roi_029\n",
      "Processing 30/205: training_set_metastatic_roi_030\n",
      "Processing 31/205: training_set_metastatic_roi_031\n",
      "Processing 32/205: training_set_metastatic_roi_032\n",
      "Processing 33/205: training_set_metastatic_roi_033\n",
      "Processing 34/205: training_set_metastatic_roi_034\n",
      "Processing 35/205: training_set_metastatic_roi_035\n",
      "Processing 36/205: training_set_metastatic_roi_036\n",
      "Processing 37/205: training_set_metastatic_roi_037\n",
      "Processing 38/205: training_set_metastatic_roi_038\n",
      "Processing 39/205: training_set_metastatic_roi_039\n",
      "Processing 40/205: training_set_metastatic_roi_040\n",
      "Processing 41/205: training_set_metastatic_roi_041\n",
      "Processing 42/205: training_set_metastatic_roi_042\n",
      "Processing 43/205: training_set_metastatic_roi_043\n",
      "Processing 44/205: training_set_metastatic_roi_044\n",
      "Processing 45/205: training_set_metastatic_roi_045\n",
      "Processing 46/205: training_set_metastatic_roi_046\n",
      "Processing 47/205: training_set_metastatic_roi_047\n",
      "Processing 48/205: training_set_metastatic_roi_048\n",
      "Processing 49/205: training_set_metastatic_roi_049\n",
      "Processing 50/205: training_set_metastatic_roi_050\n",
      "Processing 51/205: training_set_metastatic_roi_051\n",
      "Processing 52/205: training_set_metastatic_roi_052\n",
      "Processing 53/205: training_set_metastatic_roi_053\n",
      "Processing 54/205: training_set_metastatic_roi_054\n",
      "Processing 55/205: training_set_metastatic_roi_055\n",
      "Processing 56/205: training_set_metastatic_roi_056\n",
      "Processing 57/205: training_set_metastatic_roi_057\n",
      "Processing 58/205: training_set_metastatic_roi_058\n",
      "Processing 59/205: training_set_metastatic_roi_059\n",
      "Processing 60/205: training_set_metastatic_roi_060\n",
      "Processing 61/205: training_set_metastatic_roi_061\n",
      "Processing 62/205: training_set_metastatic_roi_062\n",
      "Processing 63/205: training_set_metastatic_roi_063\n",
      "Processing 64/205: training_set_metastatic_roi_064\n",
      "Processing 65/205: training_set_metastatic_roi_065\n",
      "Processing 66/205: training_set_metastatic_roi_066\n",
      "Processing 67/205: training_set_metastatic_roi_067\n",
      "Processing 68/205: training_set_metastatic_roi_068\n",
      "Processing 69/205: training_set_metastatic_roi_069\n",
      "Processing 70/205: training_set_metastatic_roi_070\n",
      "Processing 71/205: training_set_metastatic_roi_071\n",
      "Processing 72/205: training_set_metastatic_roi_072\n",
      "Processing 73/205: training_set_metastatic_roi_073\n",
      "Processing 74/205: training_set_metastatic_roi_074\n",
      "Processing 75/205: training_set_metastatic_roi_075\n",
      "Processing 76/205: training_set_metastatic_roi_076\n",
      "Processing 77/205: training_set_metastatic_roi_077\n",
      "Processing 78/205: training_set_metastatic_roi_078\n",
      "Processing 79/205: training_set_metastatic_roi_079\n",
      "Processing 80/205: training_set_metastatic_roi_080\n",
      "Processing 81/205: training_set_metastatic_roi_081\n",
      "Processing 82/205: training_set_metastatic_roi_082\n",
      "Processing 83/205: training_set_metastatic_roi_083\n",
      "Processing 84/205: training_set_metastatic_roi_084\n",
      "Processing 85/205: training_set_metastatic_roi_085\n",
      "Processing 86/205: training_set_metastatic_roi_086\n",
      "Processing 87/205: training_set_metastatic_roi_087\n",
      "Processing 88/205: training_set_metastatic_roi_088\n",
      "Processing 89/205: training_set_metastatic_roi_089\n",
      "Processing 90/205: training_set_metastatic_roi_090\n",
      "Processing 91/205: training_set_metastatic_roi_091\n",
      "Processing 92/205: training_set_metastatic_roi_092\n",
      "Processing 93/205: training_set_metastatic_roi_093\n",
      "Processing 94/205: training_set_metastatic_roi_094\n",
      "Processing 95/205: training_set_metastatic_roi_095\n",
      "Processing 96/205: training_set_metastatic_roi_096\n",
      "Processing 97/205: training_set_metastatic_roi_097\n",
      "Processing 98/205: training_set_metastatic_roi_098\n",
      "Processing 99/205: training_set_metastatic_roi_099\n",
      "Processing 100/205: training_set_metastatic_roi_100\n",
      "Processing 101/205: training_set_metastatic_roi_101\n",
      "Processing 102/205: training_set_metastatic_roi_102\n",
      "Processing 103/205: training_set_primary_roi_001\n",
      "Processing 104/205: training_set_primary_roi_002\n",
      "Processing 105/205: training_set_primary_roi_003\n",
      "Processing 106/205: training_set_primary_roi_004\n",
      "Processing 107/205: training_set_primary_roi_005\n",
      "Processing 108/205: training_set_primary_roi_006\n",
      "Processing 109/205: training_set_primary_roi_007\n",
      "Processing 110/205: training_set_primary_roi_008\n",
      "Processing 111/205: training_set_primary_roi_009\n",
      "Processing 112/205: training_set_primary_roi_010\n",
      "Processing 113/205: training_set_primary_roi_011\n",
      "Processing 114/205: training_set_primary_roi_012\n",
      "Processing 115/205: training_set_primary_roi_013\n",
      "Processing 116/205: training_set_primary_roi_014\n",
      "Processing 117/205: training_set_primary_roi_015\n",
      "Processing 118/205: training_set_primary_roi_016\n",
      "Processing 119/205: training_set_primary_roi_017\n",
      "Processing 120/205: training_set_primary_roi_018\n",
      "Processing 121/205: training_set_primary_roi_019\n",
      "Processing 122/205: training_set_primary_roi_020\n",
      "Processing 123/205: training_set_primary_roi_021\n",
      "Processing 124/205: training_set_primary_roi_022\n",
      "Processing 125/205: training_set_primary_roi_023\n",
      "Processing 126/205: training_set_primary_roi_024\n",
      "Processing 127/205: training_set_primary_roi_025\n",
      "Processing 128/205: training_set_primary_roi_026\n",
      "Processing 129/205: training_set_primary_roi_027\n",
      "Processing 130/205: training_set_primary_roi_028\n",
      "Processing 131/205: training_set_primary_roi_029\n",
      "Processing 132/205: training_set_primary_roi_030\n",
      "Processing 133/205: training_set_primary_roi_031\n",
      "Processing 134/205: training_set_primary_roi_032\n",
      "Processing 135/205: training_set_primary_roi_033\n",
      "Processing 136/205: training_set_primary_roi_034\n",
      "Processing 137/205: training_set_primary_roi_035\n",
      "Processing 138/205: training_set_primary_roi_036\n",
      "Processing 139/205: training_set_primary_roi_037\n",
      "Processing 140/205: training_set_primary_roi_038\n",
      "Processing 141/205: training_set_primary_roi_039\n",
      "Processing 142/205: training_set_primary_roi_040\n",
      "Processing 143/205: training_set_primary_roi_041\n",
      "Processing 144/205: training_set_primary_roi_042\n",
      "Processing 145/205: training_set_primary_roi_043\n",
      "Processing 146/205: training_set_primary_roi_044\n",
      "Processing 147/205: training_set_primary_roi_045\n",
      "Processing 148/205: training_set_primary_roi_046\n",
      "Processing 149/205: training_set_primary_roi_047\n",
      "Processing 150/205: training_set_primary_roi_048\n",
      "Processing 151/205: training_set_primary_roi_049\n",
      "Processing 152/205: training_set_primary_roi_050\n",
      "Processing 153/205: training_set_primary_roi_051\n",
      "Processing 154/205: training_set_primary_roi_052\n",
      "Processing 155/205: training_set_primary_roi_053\n",
      "Processing 156/205: training_set_primary_roi_054\n",
      "Processing 157/205: training_set_primary_roi_055\n",
      "Processing 158/205: training_set_primary_roi_056\n",
      "Processing 159/205: training_set_primary_roi_057\n",
      "Processing 160/205: training_set_primary_roi_058\n",
      "Processing 161/205: training_set_primary_roi_059\n",
      "Processing 162/205: training_set_primary_roi_060\n",
      "Processing 163/205: training_set_primary_roi_061\n",
      "Processing 164/205: training_set_primary_roi_062\n",
      "Processing 165/205: training_set_primary_roi_063\n",
      "Processing 166/205: training_set_primary_roi_064\n",
      "Processing 167/205: training_set_primary_roi_065\n",
      "Processing 168/205: training_set_primary_roi_066\n",
      "Processing 169/205: training_set_primary_roi_067\n",
      "Processing 170/205: training_set_primary_roi_068\n",
      "Processing 171/205: training_set_primary_roi_069\n",
      "Processing 172/205: training_set_primary_roi_070\n",
      "Processing 173/205: training_set_primary_roi_071\n",
      "Processing 174/205: training_set_primary_roi_072\n",
      "Processing 175/205: training_set_primary_roi_073\n",
      "Processing 176/205: training_set_primary_roi_074\n",
      "Processing 177/205: training_set_primary_roi_075\n",
      "Processing 178/205: training_set_primary_roi_076\n",
      "Processing 179/205: training_set_primary_roi_077\n",
      "Processing 180/205: training_set_primary_roi_078\n",
      "Processing 181/205: training_set_primary_roi_079\n",
      "Processing 182/205: training_set_primary_roi_080\n",
      "Processing 183/205: training_set_primary_roi_081\n",
      "Processing 184/205: training_set_primary_roi_082\n",
      "Processing 185/205: training_set_primary_roi_083\n",
      "Processing 186/205: training_set_primary_roi_084\n",
      "Processing 187/205: training_set_primary_roi_085\n",
      "Processing 188/205: training_set_primary_roi_086\n",
      "Processing 189/205: training_set_primary_roi_087\n",
      "Processing 190/205: training_set_primary_roi_088\n",
      "Processing 191/205: training_set_primary_roi_089\n",
      "Processing 192/205: training_set_primary_roi_090\n",
      "Processing 193/205: training_set_primary_roi_091\n",
      "Processing 194/205: training_set_primary_roi_092\n",
      "Processing 195/205: training_set_primary_roi_093\n",
      "Processing 196/205: training_set_primary_roi_094\n",
      "Processing 197/205: training_set_primary_roi_095\n",
      "Processing 198/205: training_set_primary_roi_096\n",
      "Processing 199/205: training_set_primary_roi_097\n",
      "Processing 200/205: training_set_primary_roi_098\n",
      "Processing 201/205: training_set_primary_roi_099\n",
      "Processing 202/205: training_set_primary_roi_100\n",
      "Processing 203/205: training_set_primary_roi_101\n",
      "Processing 204/205: training_set_primary_roi_102\n",
      "Processing 205/205: training_set_primary_roi_103\n",
      "Finished processing track1. Total patches saved: 20500\n"
     ]
    }
   ],
   "source": [
    "# Set output directories\n",
    "track1_patches_dir = \"./processed_data/track1_patches\"\n",
    "track2_patches_dir = \"./processed_data/track2_patches\"\n",
    "\n",
    "# Process data for Track 1 - uncomment to run\n",
    "process_dataset(track1_patches_dir, track1_mapping, \"track1\")\n",
    "\n",
    "# Process data for Track 2 - uncomment to run\n",
    "# process_dataset(track2_patches_dir, track2_mapping, \"track2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf703b",
   "metadata": {},
   "source": [
    "### Split Data into Training and Validation Sets\n",
    "\n",
    "Next, we'll split our processed patches into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95c4a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_train_val(input_dir, train_dir, val_dir, val_ratio=0.2):\n",
    "    \"\"\"Split patches into training and validation sets.\"\"\"\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    \n",
    "    all_patches = glob.glob(os.path.join(input_dir, \"*.npy\"))\n",
    "    \n",
    "    # Group patches by ROI ID to ensure patches from same ROI stay in same split\n",
    "    roi_groups = {}\n",
    "    for patch_path in all_patches:\n",
    "        patch_name = os.path.basename(patch_path)\n",
    "        roi_id = patch_name.split('_patch_')[0]\n",
    "        if roi_id not in roi_groups:\n",
    "            roi_groups[roi_id] = []\n",
    "        roi_groups[roi_id].append(patch_path)\n",
    "    \n",
    "    # Split ROIs into train and validation\n",
    "    all_rois = list(roi_groups.keys())\n",
    "    random.shuffle(all_rois)\n",
    "    val_size = int(len(all_rois) * val_ratio)\n",
    "    val_rois = all_rois[:val_size]\n",
    "    train_rois = all_rois[val_size:]\n",
    "    \n",
    "    # Copy files\n",
    "    for roi in train_rois:\n",
    "        for patch_path in roi_groups[roi]:\n",
    "            patch_name = os.path.basename(patch_path)\n",
    "            shutil.copy(patch_path, os.path.join(train_dir, patch_name))\n",
    "    \n",
    "    for roi in val_rois:\n",
    "        for patch_path in roi_groups[roi]:\n",
    "            patch_name = os.path.basename(patch_path)\n",
    "            shutil.copy(patch_path, os.path.join(val_dir, patch_name))\n",
    "    \n",
    "    print(f\"Training set: {len(glob.glob(os.path.join(train_dir, '*.npy')))} patches from {len(train_rois)} ROIs\")\n",
    "    print(f\"Validation set: {len(glob.glob(os.path.join(val_dir, '*.npy')))} patches from {len(val_rois)} ROIs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9834697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 19600 patches from 164 ROIs\n",
      "Validation set: 7300 patches from 41 ROIs\n"
     ]
    }
   ],
   "source": [
    "# Set directories for train/val splits\n",
    "track1_train_dir = \"./processed_data/track1_train\"\n",
    "track1_val_dir = \"./processed_data/track1_val\"\n",
    "track2_train_dir = \"./processed_data/track2_train\"\n",
    "track2_val_dir = \"./processed_data/track2_val\"\n",
    "\n",
    "# Uncomment to run the splits\n",
    "split_train_val(track1_patches_dir, track1_train_dir, track1_val_dir)\n",
    "# split_train_val(track2_patches_dir, track2_train_dir, track2_val_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29314c",
   "metadata": {},
   "source": [
    "## 4. Configure HoVer-Net for PUMA Dataset\n",
    "\n",
    "Now we need to create custom configuration files for HoVer-Net that specify our PUMA dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8229c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PUMA dataset class for Track 1 (3 classes)\n",
    "puma_track1_definition = \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_puma_track1():\n",
    "    return {\n",
    "        'hash': 'puma_track1',\n",
    "        'name': 'puma_track1',\n",
    "        'import_prep_func': None,\n",
    "        'input_path': './processed_data/track1_train/',\n",
    "        'split_info': {\n",
    "            'train': {\n",
    "                'input': './processed_data/track1_train/*.npy',\n",
    "            },\n",
    "            'valid': {\n",
    "                'input': './processed_data/track1_val/*.npy',\n",
    "            },\n",
    "        },\n",
    "        'type_info': {\n",
    "            0: ['background', [0, 0, 0]],\n",
    "            1: ['tumor', [255, 0, 0]],\n",
    "            2: ['TILs', [0, 255, 0]],\n",
    "            3: ['other', [0, 0, 255]],\n",
    "        },\n",
    "    }\n",
    "\n",
    "def get_puma_track2():\n",
    "    return {\n",
    "        'hash': 'puma_track2',\n",
    "        'name': 'puma_track2',\n",
    "        'import_prep_func': None,\n",
    "        'input_path': './processed_data/track2_train/',\n",
    "        'split_info': {\n",
    "            'train': {\n",
    "                'input': './processed_data/track2_train/*.npy',\n",
    "            },\n",
    "            'valid': {\n",
    "                'input': './processed_data/track2_val/*.npy',\n",
    "            },\n",
    "        },\n",
    "        'type_info': {\n",
    "            0: ['background', [0, 0, 0]],\n",
    "            1: ['tumor', [255, 0, 0]],\n",
    "            2: ['lymphocyte', [0, 255, 0]],\n",
    "            3: ['plasma', [0, 0, 255]],\n",
    "            4: ['histiocyte', [255, 255, 0]],\n",
    "            5: ['melanophage', [255, 0, 255]],\n",
    "            6: ['neutrophil', [0, 255, 255]],\n",
    "            7: ['stromal', [128, 0, 0]],\n",
    "            8: ['epithelium', [0, 128, 0]],\n",
    "            9: ['endothelium', [0, 0, 128]],\n",
    "            10: ['apoptotic', [128, 128, 128]],\n",
    "        },\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "# Save the custom dataset definitions\n",
    "with open(\"./hover_net/custom_puma_dataset.py\", \"w\") as f:\n",
    "    f.write(puma_track1_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2947668",
   "metadata": {},
   "source": [
    "### Modify HoVer-Net Dataset Configuration\n",
    "\n",
    "We need to register our custom dataset in the main dataset.py file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01c700a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUMA datasets already registered in dataset.py\n"
     ]
    }
   ],
   "source": [
    "# Function to update dataset.py to include our custom dataset\n",
    "def update_dataset_file():\n",
    "    dataset_path = \"./hover_net/dataset.py\"\n",
    "    \n",
    "    # Read the original file\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Check if our dataset is already registered\n",
    "    if \"from custom_puma_dataset import get_puma_track1, get_puma_track2\" in content:\n",
    "        print(\"PUMA datasets already registered in dataset.py\")\n",
    "        return\n",
    "    \n",
    "    # Add import at the beginning\n",
    "    import_line = \"from custom_puma_dataset import get_puma_track1, get_puma_track2\\n\"\n",
    "    # Look for other imports and add after them\n",
    "    import_block_end = content.find(\"def get_dataset\")\n",
    "    if import_block_end > 0:\n",
    "        content = content[:import_block_end] + import_line + content[import_block_end:]\n",
    "    \n",
    "    # Find the dataset_info dictionary and add our datasets\n",
    "    dataset_dict_start = content.find(\"dataset_info = {\")\n",
    "    if dataset_dict_start > 0:\n",
    "        dataset_dict_end = content.find(\"}\", dataset_dict_start)\n",
    "        if dataset_dict_end > 0:\n",
    "            new_entries = \"\"\"\n",
    "    \"puma_track1\": get_puma_track1(),\n",
    "    \"puma_track2\": get_puma_track2(),\"\"\"\n",
    "            content = content[:dataset_dict_end] + new_entries + content[dataset_dict_end:]\n",
    "    \n",
    "    # Write back to file\n",
    "    with open(dataset_path, \"w\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    print(\"Updated dataset.py with PUMA dataset configurations\")\n",
    "\n",
    "# Update the dataset file\n",
    "update_dataset_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbb1b4",
   "metadata": {},
   "source": [
    "### Create Custom Configuration Files\n",
    "\n",
    "Now let's create a custom configuration file for PUMA training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92e8849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created custom config file at ./hover_net/puma_track1_config.py\n"
     ]
    }
   ],
   "source": [
    "def create_custom_config(track_id=1, nr_classes=4):\n",
    "    \"\"\"\n",
    "    Create a custom config file for HoVer-Net training on PUMA dataset.\n",
    "    track_id: 1 or 2 (corresponding to Track 1 or Track 2)\n",
    "    nr_classes: number of classes including background (4 for track1, 11 for track2)\n",
    "    \"\"\"\n",
    "    config_content = f\"\"\"\n",
    "import importlib\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from dataset import get_dataset\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.seed = 10\n",
    "        self.logging = True\n",
    "        self.debug = False\n",
    "        \n",
    "        model_name = \"hovernet\"\n",
    "        model_mode = \"original\"  # original or fast\n",
    "        \n",
    "        # Number of nuclear types (including background)\n",
    "        nr_type = {nr_classes}  # {nr_classes-1} classes + background\n",
    "        \n",
    "        # Whether to predict the nuclear type\n",
    "        self.type_classification = True\n",
    "        \n",
    "        # Shape information\n",
    "        aug_shape = [540, 540]  # patch shape used during augmentation\n",
    "        act_shape = [270, 270]  # patch shape used as input to network\n",
    "        out_shape = [80, 80]    # patch shape at output of network\n",
    "        \n",
    "        # Dataset name\n",
    "        self.dataset_name = \"puma_track{track_id}\"\n",
    "        \n",
    "        # Log directory for checkpoints\n",
    "        self.log_dir = \"logs/puma_track{track_id}/\"\n",
    "        \n",
    "        # Paths to training and validation patches\n",
    "        self.train_dir_list = [\n",
    "            \"./processed_data/track{track_id}_train\"\n",
    "        ]\n",
    "        self.valid_dir_list = [\n",
    "            \"./processed_data/track{track_id}_val\"\n",
    "        ]\n",
    "        \n",
    "        self.shape_info = {{\n",
    "            \"train\": {{\n",
    "                \"input_shape\": act_shape,\n",
    "                \"mask_shape\": out_shape,\n",
    "            }},\n",
    "            \"valid\": {{\n",
    "                \"input_shape\": act_shape,\n",
    "                \"mask_shape\": out_shape,\n",
    "            }},\n",
    "        }}\n",
    "        \n",
    "        # Parse config to the running state and set up associated variables\n",
    "        self.dataset = get_dataset(self.dataset_name)\n",
    "        module = importlib.import_module(\n",
    "            \"models.%s.opt\" % model_name\n",
    "        )\n",
    "        self.model_config = module.get_config(nr_type, model_mode)\n",
    "    \"\"\"\n",
    "    \n",
    "    config_path = f\"./hover_net/puma_track{track_id}_config.py\"\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    print(f\"Created custom config file at {config_path}\")\n",
    "\n",
    "# Create config files for both tracks\n",
    "create_custom_config(track_id=1, nr_classes=4)  # Track 1: background + 3 classes\n",
    "# create_custom_config(track_id=2, nr_classes=11)  # Track 2: background + 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a4510",
   "metadata": {},
   "source": [
    "## 5. Training HoVer-Net\n",
    "\n",
    "With our data prepared and configuration files created, we're now ready to train HoVer-Net for the PUMA challenge. We'll train separate models for Track 1 and Track 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99cb749c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 1 Training Files: 19600\n",
      "Track 1 Validation Files: 7300\n",
      "Track 2 Training Files: 0\n",
      "Track 2 Validation Files: 0\n",
      "\n",
      "Sample Training File Shape: (270, 270, 6)\n",
      "Channels: RGB + Instance Mask + Type Mask\n",
      "  - RGB shape: (270, 270, 3)\n",
      "  - Instance mask shape: (270, 270)\n",
      "  - Type mask shape: (270, 270)\n",
      "\n",
      "Instance mask unique values: [255]\n",
      "Type mask unique values: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66]\n"
     ]
    }
   ],
   "source": [
    "# Check if our processed data is available\n",
    "def check_processed_data():\n",
    "    track1_train_files = glob.glob(\"./processed_data/track1_train/*.npy\")\n",
    "    track1_val_files = glob.glob(\"./processed_data/track1_val/*.npy\")\n",
    "    track2_train_files = glob.glob(\"./processed_data/track2_train/*.npy\")\n",
    "    track2_val_files = glob.glob(\"./processed_data/track2_val/*.npy\")\n",
    "    \n",
    "    print(f\"Track 1 Training Files: {len(track1_train_files)}\")\n",
    "    print(f\"Track 1 Validation Files: {len(track1_val_files)}\")\n",
    "    print(f\"Track 2 Training Files: {len(track2_train_files)}\")\n",
    "    print(f\"Track 2 Validation Files: {len(track2_val_files)}\")\n",
    "    \n",
    "    # Check a few files to see if they're properly formatted\n",
    "    if len(track1_train_files) > 0:\n",
    "        sample = np.load(track1_train_files[0])\n",
    "        print(f\"\\nSample Training File Shape: {sample.shape}\")\n",
    "        print(f\"Channels: RGB + Instance Mask + Type Mask\")\n",
    "        print(f\"  - RGB shape: {sample[..., :3].shape}\")\n",
    "        print(f\"  - Instance mask shape: {sample[..., 3].shape}\")\n",
    "        print(f\"  - Type mask shape: {sample[..., 4].shape}\")\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"\\nInstance mask unique values: {np.unique(sample[..., 3])}\")\n",
    "        print(f\"Type mask unique values: {np.unique(sample[..., 4])}\")\n",
    "\n",
    "# Check if our processed data is ready\n",
    "check_processed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fc625",
   "metadata": {},
   "source": [
    "### Run Training\n",
    "\n",
    "Now we'll set up the code to run HoVer-Net training using our custom configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac1c4927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HoVer-Net training for PUMA Track 1...\n",
      "Using GPUs: 0\n",
      "\n",
      "Command: python ./hover_net/run_train.py --gpu=0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/qberal/Developer/Cours/INSA_Rouen/S8/RL_Projet/Deep_learning_project/code/./hover_net/run_train.py\", line 37, in <module>\n",
      "    from dataloader.train_loader import FileLoader\n",
      "  File \"/Users/qberal/Developer/Cours/INSA_Rouen/S8/RL_Projet/Deep_learning_project/code/hover_net/dataloader/train_loader.py\", line 12, in <module>\n",
      "    import imgaug as ia\n",
      "  File \"/Users/qberal/Developer/Cours/INSA_Rouen/S8/RL_Projet/Deep_learning_project/code/.venv/lib/python3.12/site-packages/imgaug/__init__.py\", line 7, in <module>\n",
      "    from imgaug.imgaug import *  # pylint: disable=redefined-builtin\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/qberal/Developer/Cours/INSA_Rouen/S8/RL_Projet/Deep_learning_project/code/.venv/lib/python3.12/site-packages/imgaug/imgaug.py\", line 45, in <module>\n",
      "    NP_FLOAT_TYPES = set(np.sctypes[\"float\"])\n",
      "                         ^^^^^^^^^^\n",
      "  File \"/Users/qberal/Developer/Cours/INSA_Rouen/S8/RL_Projet/Deep_learning_project/code/.venv/lib/python3.12/site-packages/numpy/__init__.py\", line 400, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: `np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training failed with error: Command '['python', './hover_net/run_train.py', '--gpu=0']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def train_hovernet(track_id=1, gpu_ids=\"0\"):\n",
    "    \"\"\"Train HoVer-Net for a specific PUMA track.\"\"\"\n",
    "    # Set environment variable for GPUs\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids\n",
    "    \n",
    "    # Command to run the training script\n",
    "    cmd = [\n",
    "        \"python\", \n",
    "        f\"./hover_net/run_train.py\", \n",
    "        f\"--gpu={gpu_ids}\"\n",
    "    ]\n",
    "    \n",
    "    # Make sure the config file is properly imported\n",
    "    original_config_path = \"./hover_net/config.py\"\n",
    "    custom_config_path = f\"./hover_net/puma_track{track_id}_config.py\"\n",
    "    \n",
    "    # Backup original config\n",
    "    if os.path.exists(original_config_path):\n",
    "        with open(original_config_path, \"r\") as f:\n",
    "            original_config_content = f.read()\n",
    "        \n",
    "        # Create backup\n",
    "        with open(original_config_path + \".backup\", \"w\") as f:\n",
    "            f.write(original_config_content)\n",
    "    \n",
    "    # Copy our custom config to the main config.py file\n",
    "    with open(custom_config_path, \"r\") as f:\n",
    "        custom_config_content = f.read()\n",
    "    \n",
    "    with open(original_config_path, \"w\") as f:\n",
    "        f.write(custom_config_content)\n",
    "    \n",
    "    print(f\"Starting HoVer-Net training for PUMA Track {track_id}...\")\n",
    "    print(f\"Using GPUs: {gpu_ids}\")\n",
    "    print(f\"\\nCommand: {' '.join(cmd)}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Run the training script\n",
    "        # In a real execution, you would uncomment the following:\n",
    "        result = subprocess.run(cmd, check=True)\n",
    "        print(f\"Training completed with exit code {result.returncode}\")\n",
    "        print(\"[Simulated] Training would start here. In a real execution, uncomment the subprocess.run call.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Training failed with error: {e}\")\n",
    "    finally:\n",
    "        # Restore original config if backup exists\n",
    "        if os.path.exists(original_config_path + \".backup\"):\n",
    "            with open(original_config_path + \".backup\", \"r\") as f:\n",
    "                backup_content = f.read()\n",
    "            \n",
    "            with open(original_config_path, \"w\") as f:\n",
    "                f.write(backup_content)\n",
    "            \n",
    "            # Remove backup\n",
    "            os.remove(original_config_path + \".backup\")\n",
    "\n",
    "# Train for Track 1 (uncomment to run)\n",
    "train_hovernet(track_id=1, gpu_ids=\"0\")\n",
    "\n",
    "# Train for Track 2 (uncomment to run)\n",
    "# train_hovernet(track_id=2, gpu_ids=\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ba083",
   "metadata": {},
   "source": [
    "### Monitor Training Progress\n",
    "\n",
    "After starting the training, we can monitor its progress through TensorBoard logs. HoVer-Net training saves logs in the specified log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63130ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard (uncomment to run)\n",
    "# %tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7f6a0",
   "metadata": {},
   "source": [
    "## 6. Inference with Trained Models\n",
    "\n",
    "After training, we can use the trained models to make predictions on new images. HoVer-Net provides a script for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd4f5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(track_id=1, gpu_ids=\"0\"):\n",
    "    \"\"\"Run inference using a trained HoVer-Net model.\"\"\"\n",
    "    # Path to the trained model checkpoint\n",
    "    checkpoint_path = f\"./logs/puma_track{track_id}/hovernet_epoch=50.tar\"\n",
    "    \n",
    "    # Path to test images\n",
    "    test_dir = f\"./test_images\"\n",
    "    output_dir = f\"./results/track{track_id}\"\n",
    "    \n",
    "    # Number of classes (including background)\n",
    "    nr_types = 4 if track_id == 1 else 11\n",
    "    \n",
    "    # Command to run inference\n",
    "    cmd = [\n",
    "        \"python\", \n",
    "        \"./hover_net/run_infer.py\",\n",
    "        \"tile\",\n",
    "        f\"--gpu={gpu_ids}\",\n",
    "        f\"--nr_types={nr_types}\",\n",
    "        f\"--model_path={checkpoint_path}\",\n",
    "        \"--model_mode=original\",\n",
    "        f\"--input_dir={test_dir}\",\n",
    "        f\"--output_dir={output_dir}\",\n",
    "        \"--save_qupath\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running inference with PUMA Track {track_id} model...\")\n",
    "    print(f\"Using GPUs: {gpu_ids}\")\n",
    "    print(f\"\\nCommand: {' '.join(cmd)}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Run the inference script\n",
    "        # In a real execution, you would uncomment the following:\n",
    "        # result = subprocess.run(cmd, check=True)\n",
    "        # print(f\"Inference completed with exit code {result.returncode}\")\n",
    "        print(\"[Simulated] Inference would start here. In a real execution, uncomment the subprocess.run call.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Inference failed with error: {e}\")\n",
    "\n",
    "# Run inference with Track 1 model (uncomment to run)\n",
    "# run_inference(track_id=1, gpu_ids=\"0\")\n",
    "\n",
    "# Run inference with Track 2 model (uncomment to run)\n",
    "# run_inference(track_id=2, gpu_ids=\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca0e00",
   "metadata": {},
   "source": [
    "## 7. Evaluate Results\n",
    "\n",
    "Finally, we can evaluate the performance of our trained models on the validation set using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e396c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(track_id=1):\n",
    "    \"\"\"Evaluate the trained model using metrics from HoVer-Net.\"\"\"\n",
    "    # Path to the results directory\n",
    "    results_dir = f\"./results/track{track_id}\"\n",
    "    \n",
    "    # Command to compute metrics\n",
    "    cmd = [\n",
    "        \"python\", \n",
    "        \"./hover_net/compute_stats.py\",\n",
    "        f\"--pred_dir={results_dir}\",\n",
    "        f\"--true_dir=./processed_data/track{track_id}_val\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Computing metrics for PUMA Track {track_id} model...\")\n",
    "    print(f\"\\nCommand: {' '.join(cmd)}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Run the metrics computation script\n",
    "        # In a real execution, you would uncomment the following:\n",
    "        # result = subprocess.run(cmd, check=True)\n",
    "        # print(f\"Metrics computation completed with exit code {result.returncode}\")\n",
    "        print(\"[Simulated] Metrics computation would start here. In a real execution, uncomment the subprocess.run call.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Metrics computation failed with error: {e}\")\n",
    "\n",
    "# Evaluate Track 1 model (uncomment to run)\n",
    "# evaluate_model(track_id=1)\n",
    "\n",
    "# Evaluate Track 2 model (uncomment to run)\n",
    "# evaluate_model(track_id=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82997f1e",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the process of training HoVer-Net models for the PUMA challenge tasks. The steps included:\n",
    "\n",
    "1. Exploring and understanding the PUMA dataset\n",
    "2. Converting GeoJSON annotations to the format required by HoVer-Net\n",
    "3. Processing images and generating patches for training\n",
    "4. Creating custom configurations for HoVer-Net\n",
    "5. Training separate models for Track 1 (3 classes) and Track 2 (10 classes)\n",
    "6. Running inference on new images\n",
    "7. Evaluating model performance with metrics\n",
    "\n",
    "HoVer-Net's architecture, with its dedicated branches for segmentation and classification, is well-suited for the PUMA challenge which requires both accurate nuclei segmentation and classification.\n",
    "\n",
    "To improve results, consider the following:\n",
    "- Try different data augmentation techniques\n",
    "- Adjust learning rates and training epochs\n",
    "- Experiment with different model configurations (original vs. fast mode)\n",
    "- Explore ensemble methods by combining predictions from multiple models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
